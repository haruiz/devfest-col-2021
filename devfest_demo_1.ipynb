{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "devfest-demo-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wED22jGlFw5-"
      },
      "source": [
        "import IPython.display as idisplay\n",
        "from google.colab.output import eval_js\n",
        "import matplotlib.pyplot as plt\n",
        "from base64 import b64decode, b64encode\n",
        "import math\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2 \n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFQQjJ4EGDY1"
      },
      "source": [
        "VIDEO_JS = idisplay.Javascript('''\n",
        "async function takePhoto(quality) {\n",
        "  // Create a video and play it.\n",
        "  const video = document.createElement('video')\n",
        "  document.body.appendChild(video)\n",
        "  video.srcObject = await navigator.mediaDevices.getUserMedia({video: true})\n",
        "  await video.play()\n",
        "  // Resize the output to fit the video element.\n",
        "  google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true)\n",
        "  // Wait for video to be clicked.\n",
        "  await new Promise((resolve) => video.onclick = resolve)\n",
        "  const canvas = document.createElement('canvas')\n",
        "  canvas.width = video.videoWidth\n",
        "  canvas.height = video.videoHeight\n",
        "  canvas.getContext('2d').drawImage(video, 0, 0)\n",
        "  video.srcObject.getVideoTracks()[0].stop()\n",
        "  video.remove()\n",
        "  // return capture image\n",
        "  return canvas.toDataURL('image/jpeg', quality)\n",
        "}\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plbThacrF588"
      },
      "source": [
        "class  Camera:\n",
        "    def __init__(self, quality: float = 0.8):\n",
        "        self._quality = quality\n",
        "\n",
        "    def take_picture(self):\n",
        "        display(VIDEO_JS)\n",
        "        canvas_url = eval_js(f'takePhoto({self._quality})')\n",
        "        base64_img = canvas_url.split(',')[1]\n",
        "        buffer_img = b64decode(base64_img)        \n",
        "        img = Image.open(io.BytesIO(buffer_img))\n",
        "        return img    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQzhZXyHM6D"
      },
      "source": [
        "cam = Camera()\n",
        "input_image = cam.take_picture()\n",
        "plt.imshow(input_image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6khzxHsI41Z"
      },
      "source": [
        "API_KEY = \"AIzaSyDbwPxcyIRoRnGox28J5WtEGsFQYYHCqag\"\n",
        "GCP_VISION_API_ENDPOINT = f\"https://vision.googleapis.com/v1/images:annotate?key={API_KEY}\"\n",
        "\n",
        "class Robot:\n",
        "    def __init__(self, name: str):\n",
        "        self.sensors = {\n",
        "            \"camera\": Camera()\n",
        "        }\n",
        "\n",
        "    def detect_face(self):\n",
        "        pil_image = self.sensors[\"camera\"].take_picture()       \n",
        "        numpy_image = np.asarray(pil_image) \n",
        "        success, encoded_image = cv2.imencode('.jpg', numpy_image)\n",
        "        encoded_image_bytes = encoded_image.tobytes()\n",
        "        encoded_image_base64 = b64encode(encoded_image_bytes)\n",
        "        encoded_image_base64 = encoded_image_base64.decode(\"utf-8\")        \n",
        "        response = requests.post(\n",
        "            url=GCP_VISION_API_ENDPOINT, \n",
        "            json={\n",
        "                \"requests\": [{\n",
        "                \"image\": {\n",
        "                    \"content\": encoded_image_base64\n",
        "                },\n",
        "                \"features\": [{\n",
        "                    \"maxResults\": 10,\n",
        "                    \"type\": \"FACE_DETECTION\"\n",
        "                }]\n",
        "            }]},\n",
        "            headers={ \n",
        "                'Content-Type': 'application/json'\n",
        "            })\n",
        "        return pil_image, response.json()    \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QNZNlngKCdt"
      },
      "source": [
        "my_robot = Robot(name=\"wall-e\")\n",
        "pil_image, api_response = my_robot.detect_face()\n",
        "print(api_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps4aWQ5KKcTq"
      },
      "source": [
        "face_annotations = api_response[\"responses\"][0][\"faceAnnotations\"]\n",
        "num_faces = len(face_annotations)\n",
        "canvas_image = pil_image.copy()\n",
        "image_drawer = ImageDraw.Draw(canvas_image)\n",
        "for face_idx in range(0, num_faces):    \n",
        "    face_location = face_annotations[face_idx][\"boundingPoly\"]       \n",
        "    points = [(point[\"x\"], point[\"y\"]) for point in face_location[\"vertices\"]]\n",
        "    image_drawer.polygon(points, outline =\"red\")\n",
        "canvas_image"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}